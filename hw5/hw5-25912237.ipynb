{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All code can be very easily ran with python, you just have to import the classes from the inner `tree` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy.random import choice\n",
    "from tree.tree import DecisionTree, RandomForest\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_list = ['pain', 'private', 'bank', 'money', \n",
    "                'drug', 'spam', 'prescription', \n",
    "                'creative', 'height', 'featured', \n",
    "                'differ', 'width', 'other', 'energy', \n",
    "                'business', 'message', 'volumes', 'revision', \n",
    "                'path', 'meter', 'memo', 'planning', 'pleased', \n",
    "                'record', 'out' ,';','$','#','!','(','[','&']\n",
    "feature_dict = dict(zip(range(len(feature_list)), feature_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See Below\n",
    "\n",
    "# See Below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spam = scipy.io.loadmat(\"spam-dataset/spam_data.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spam_raw_train_X = spam['training_data']\n",
    "spam_raw_train_y = spam['training_labels'].reshape((5172,))\n",
    "spam_raw_test_X = spam['test_data']\n",
    "shuff = choice(len(spam_raw_train_X), len(spam_raw_train_X), False)\n",
    "spam_raw_train_X = spam['training_data'][shuff]\n",
    "spam_raw_train_y = spam['training_labels'].reshape((5172,))[shuff]\n",
    "spam_raw_test_X = spam['test_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spamTX, spamTestX, spamTy, spamTesty = train_test_split(\n",
    "    spam_raw_train_X, spam_raw_train_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137 1035\n"
     ]
    }
   ],
   "source": [
    "print(len(spamTX), len(spamTestX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. \n",
    "\n",
    "No extra features were used for either decision trees or random forest with spam.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "dt = DecisionTree({\n",
    "        \"max_depth\":100, # this is > max depth possible\n",
    "        \"min_points\": 2\n",
    "    })\n",
    "dt.train(spamTX, spamTy)\n",
    "scored = dt.score(spamTestX, spamTesty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Validation Error Rate: 0.811594202899\n"
     ]
    }
   ],
   "source": [
    "print(\"Decision Tree Validation Error Rate:\", sum(scored)/len(scored))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spam_out = pd.DataFrame(dt.predict(spam_raw_test_X)).reset_index()\n",
    "spam_out.columns = ['Id', 'Category']\n",
    "spam_out.Id = spam_out.Id + 1\n",
    "spam_out.to_csv(\"dt_spam_prediction.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!  <=  0.0\n",
      "meter  <=  0.0\n",
      "(  <=  0.0\n",
      "volumes  <=  0.0\n",
      "&  <=  0.0\n",
      "pain  <=  0.0\n",
      ";  <=  0.0\n",
      "[  <=  0.0\n",
      "prescription  <=  0.0\n",
      "energy  <=  1.0\n",
      "path  <=  0.0\n",
      "memo  <=  0.0\n",
      "bank  <=  1.0\n",
      "spam  <=  0.0\n",
      "differ  <=  0.0\n",
      "drug  <=  0.0\n",
      "planning  <=  0.0\n",
      "#  <=  1.0\n",
      "revision  <=  0.0\n",
      "pleased  <=  0.0\n",
      "$  <=  6.0\n",
      "business  <=  0.0\n",
      "message  <=  0.0\n",
      "featured  <=  0.0\n",
      "width  <=  0.0\n",
      "other  <=  2.0\n",
      "out  <=  2.0\n",
      "other  <=  1.0\n",
      "bank  <=  0.0\n",
      "out  <=  0.0\n",
      "money  <=  1.0\n",
      "$  <=  2.0\n",
      "$  <=  1.0\n",
      "money  <=  0.0\n",
      "$  <=  0.0\n",
      "private  <=  0.0\n",
      "other  <=  0.0\n",
      "#  >  0.0\n",
      "End of tree, outputting label 0\n",
      "Actual: 0\n"
     ]
    }
   ],
   "source": [
    "dt.get_splits(spamTX[0].reshape(1, len(spamTX[0])), feature_list)\n",
    "print(\"Actual:\", spamTy[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "rf = RandomForest({\n",
    "        'ntrees': 50,\n",
    "        },{\n",
    "        \"max_depth\": 100,\n",
    "        \"min_points\": 2,\n",
    "        'random_subset': True,\n",
    "        'subset_size': 8\n",
    "        })\n",
    "print(rf)\n",
    "rf.train(spamTX, spamTy)\n",
    "scored = rf.score(spamTestX, spamTesty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. (part 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest. 50 Trees\n",
      "Training tree number 0\n",
      "Training tree number 20\n",
      "Training tree number 40\n",
      "trained all trees\n",
      "Random Forest Validation Error Rate: 0.833816425121\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest Validation Error Rate:\", sum(scored)/len(scored))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spam_out = pd.DataFrame(rf.predict(spam_raw_test_X)).reset_index()\n",
    "spam_out.columns = ['Id', 'Category']\n",
    "spam_out.Id = spam_out.Id + 1\n",
    "spam_out.to_csv(\"rf_spam_prediction.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "money split on 0.0 with  50 trees\n",
      "( split on 0.0 with  50 trees\n",
      "[ split on 0.0 with  50 trees\n",
      "spam split on 0.0 with  50 trees\n",
      "& split on 0.0 with  50 trees\n",
      "prescription split on 0.0 with  50 trees\n",
      "volumes split on 0.0 with  50 trees\n",
      "$ split on 1.0 with  50 trees\n",
      "other split on 0.0 with  50 trees\n",
      "revision split on 0.0 with  50 trees\n",
      "energy split on 0.0 with  50 trees\n",
      "pain split on 0.0 with  50 trees\n",
      "business split on 0.0 with  50 trees\n",
      "meter split on 0.0 with  50 trees\n",
      "out split on 0.0 with  50 trees\n",
      "message split on 0.0 with  50 trees\n",
      "; split on 0.0 with  50 trees\n",
      "$ split on 0.0 with  50 trees\n",
      "# split on 0.0 with  50 trees\n",
      "! split on 0.0 with  50 trees\n",
      "drug split on 0.0 with  49 trees\n",
      "differ split on 0.0 with  49 trees\n",
      "( split on 1.0 with  49 trees\n",
      "path split on 0.0 with  49 trees\n",
      "memo split on 0.0 with  49 trees\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "for (feat, val), count in Counter(rf.get_splits()).most_common(25):\n",
    "    print(feature_dict[feat], \"split on\", val, \"with \", count, \"trees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Best Random Forest Spam Submission on Kaggle: 0.78108"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Census "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "census = pd.read_csv(\"census-dataset/data.csv\", na_values=[\"?\"])\n",
    "census_test = pd.read_csv(\"census-dataset/test_data.csv\", na_values=[\"?\"])\n",
    "categoricals = [\"workclass\", \"education\", \"marital-status\", \n",
    "                \"occupation\", \"relationship\", \"race\", \"sex\", \n",
    "                \"native-country\"]\n",
    "non_categoricals = list(\n",
    "    set(census.columns\n",
    "       ).difference(set(categoricals)).difference(set([\"label\"])))\n",
    "# all we're doing is getting all non-categorical variables into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "{'native-country_Holand-Netherlands'}\n"
     ]
    }
   ],
   "source": [
    "print(set(census[non_categoricals].columns\n",
    "         ).difference(\n",
    "        set(census_test[non_categoricals].columns)))\n",
    "print(\n",
    "    set(pd.get_dummies(census[categoricals]).columns).difference(\n",
    "    set(pd.get_dummies(census_test[categoricals]).columns))\n",
    ")# we can see that we get an extra variable in the census,\n",
    "# that's obviously problematic and will throw off our computations\n",
    "# I'm going to manually add that column to our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "census_X = pd.concat(\n",
    "    [census[non_categoricals], \n",
    "     pd.get_dummies(census[categoricals])], \n",
    "    axis=1)\n",
    "census_y = census.label.values\n",
    "census_X = census_X.reindex_axis(sorted(census_X.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = pd.get_dummies(census_test[categoricals])\n",
    "temp['native-country_Holand-Netherlands'] = 0\n",
    "census_test_X = pd.concat([census_test[non_categoricals], temp], axis=1)\n",
    "census_test_X = census_test_X.reindex_axis(sorted(census_test_X.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "censusTX, censusTestX, censusTy, censusTesty = train_test_split(\n",
    "    census_X.values, census_y, test_size=.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "print(set(census_X.columns).difference(set(census_test_X.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21270, 105) (21270,)\n",
      "(11454, 105)\n"
     ]
    }
   ],
   "source": [
    "print(censusTX.shape, censusTy.shape) \n",
    "print(censusTestX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. \n",
    "\n",
    "The pre-processing I performed was just 'dummifying' the variables. I just convert them into columns for the respective categorical feature. So that every possible value in relationship becomes its own [sparse] feature column. I only handled categorical variables by creating a new column (essentially treating them as a different category)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Census Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "dt2 = DecisionTree({\n",
    "        \"max_depth\":1000,\n",
    "        \"min_points\": 15\n",
    "    })\n",
    "dt2.train(censusTX, censusTy)\n",
    "scored = dt2.score(censusTestX, censusTesty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Decision Tree Validation Error Rate: 0.845468831849\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Decision Tree Validation Error Rate:\", sum(scored)/len(scored))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "census_out = pd.DataFrame(dt2.predict(census_test_X.values)).reset_index()\n",
    "census_out.columns = ['Id', 'Category']\n",
    "census_out.Id = census_out.Id + 1\n",
    "census_out.to_csv(\"dt_census_prediction.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_dict = dict(zip(range(len(census_X.columns)), census_X.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marital-status_Widowed  <=  0.0\n",
      "education_Some-college  <=  0.0\n",
      "native-country_Cambodia  <=  0.0\n",
      "education_Masters  <=  0.0\n",
      "native-country_China  <=  0.0\n",
      "age  >  0.0\n",
      "End of tree, outputting label 1\n",
      "Actual: 0\n"
     ]
    }
   ],
   "source": [
    "dt.get_splits(censusTX[0].reshape(1, len(censusTX[0])), feature_dict)\n",
    "print(\"Actual:\", spamTy[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Census Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest. 50 Trees\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "rf2 = RandomForest({\n",
    "        'ntrees': 50,\n",
    "        },{\n",
    "        \"max_depth\": 100,\n",
    "        \"min_points\": 15,\n",
    "        'random_subset': True,\n",
    "        'subset_size': 15\n",
    "        })\n",
    "print(rf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tree number 0\n",
      "Training tree number 20\n",
      "Training tree number 40\n",
      "trained all trees\n"
     ]
    }
   ],
   "source": [
    "rf2.train(censusTX, censusTy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. (part 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest Validation Error Rate: 0.858302776323\n"
     ]
    }
   ],
   "source": [
    "scored = rf2.score(censusTestX, censusTesty)\n",
    "print(\"Best Random Forest Validation Error Rate:\", sum(scored)/len(scored))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Random Forest Kaggle: 0.85395"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age split on 28.0 with count 50\n",
      "occupation_Sales split on 0.0 with count 50\n",
      "education_Doctorate split on 0.0 with count 50\n",
      "education-num split on 11.0 with count 50\n",
      "age split on 24.0 with count 50\n",
      "native-country_Cuba split on 0.0 with count 50\n",
      "education_12th split on 0.0 with count 50\n",
      "education-num split on 10.0 with count 50\n",
      "relationship_Unmarried split on 0.0 with count 50\n",
      "education_Some-college split on 0.0 with count 50\n",
      "age split on 29.0 with count 50\n",
      "workclass_Self-emp-not-inc split on 0.0 with count 50\n",
      "education_Prof-school split on 0.0 with count 50\n",
      "relationship_Other-relative split on 0.0 with count 50\n",
      "hours-per-week split on 48.0 with count 50\n",
      "occupation_Machine-op-inspct split on 0.0 with count 50\n",
      "capital-loss split on 0.0 with count 50\n",
      "age split on 36.0 with count 50\n",
      "relationship_Husband split on 0.0 with count 50\n",
      "occupation_Farming-fishing split on 0.0 with count 50\n",
      "workclass_State-gov split on 0.0 with count 50\n",
      "occupation_Craft-repair split on 0.0 with count 50\n",
      "workclass_Self-emp-inc split on 0.0 with count 50\n",
      "marital-status_Separated split on 0.0 with count 50\n",
      "race_Black split on 0.0 with count 50\n",
      "education_Assoc-voc split on 0.0 with count 50\n",
      "age split on 46.0 with count 50\n",
      "education-num split on 12.0 with count 50\n",
      "race_Amer-Indian-Eskimo split on 0.0 with count 50\n",
      "native-country_Mexico split on 0.0 with count 50\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "for (feat, val), count in Counter(rf2.get_splits()).most_common(30):\n",
    "    print(feature_dict[feat], \"split on\", val, \"with count\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "census_out = pd.DataFrame(rf2.predict(census_test_X.values)).reset_index()\n",
    "census_out.columns = ['Id', 'Category']\n",
    "census_out.Id = census_out.Id + 1\n",
    "census_out.to_csv(\"rf_census_prediction.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 5.\n",
    "\n",
    "## a.\n",
    "\n",
    "The decision tree techniques I used were pretty straightforward. I implemented stopping criteria which you can see as the parameters passed into the creation criteria for the decision tree (and as the second parameter dictionary in the Random Forest). The two stopping criteria were:\n",
    "1. Total depth\n",
    "2. Total number of values required for a split\n",
    "\n",
    "I experimented with feature selection in the decision trees and got approximately the same kind of results. I didn't implement heuristics for faster training or complex decision boundaries, pruning, or adaboost. I didn't use cross validation.\n",
    "\n",
    "## b.\n",
    "\n",
    "My random forest implementation is a straightforward extension of the decision tree classifier mentioned above except that bagging is automatic as was feature selection as a random subset of features of a parameterized length (that you can see as passed into the random forest class creation).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
