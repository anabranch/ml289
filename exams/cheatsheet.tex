% Created 2015-10-27 Tue 09:59
\documentclass[a4paper,twoside,twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage{wasysym}
\usepackage{amssymb}
\usepackage{hyperref}
\tolerance=1000
\usepackage[margin=0.1in]{geometry}
\date{}
\title{}
\hypersetup{
  pdfkeywords={},
  pdfsubject={},
  pdfcreator={Emacs 24.4.1 (Org mode 8.2.10)}}
\begin{document}

\newpage
\newpage
\newpage

\section*{Matrix Identities}
\label{sec-1}
\setlength{\parindent}{0pt}
{\scriptsize
\begin{tabular}{ c c }
(A^{-1})^T = (A^T)^{-1}$ & (AB)^{-1} = B^{-1}A^{-1} \\
TR(A) = \Sigma eigenvalues$ & |A| = \Product eigenvalues \\
$\tfrac{\partial TR(A)}{\partial A} = I$ & $\tfrac{\partial TR(XA)}{\partial X} = \tfrac{\partial TR(AX)}{\partial X} = A^T$ \\
$\tfrac{\partial TR(X^TAX }{\partial X} = (A + A^T)X$ &
$\tfrac{\partial ln|X| }{\partial X } = X^{-T}$ \\
$\tfrac{\partial a^T x }{\partial x} = \tfrac{\partial x^T a }{\partial x} = a$ &
$\tfrac{\partial x^T A x}{\partial x} = (A + A^T)x$ \\
$\tfrac{\partial a^T X b }{\partial X } = ab^T$ & $\tfrac{\partial a^T X^T b }{\partial X} = ba^T$ \\

$\tfrac{\partial a^T X a }{\partial X } = \tfrac{\partial a^T X^T a }{\partial X } = aa^T$ & $\tfrac{\partial a^T X^T C X b }{\partial X } = C^TXab^T + CXba^T$ \\

\end{tabular}

\tfrac{\partial |X^TAX| }{\partial X} = |X^TAX|(AX(X^TAX)^{-1} + A^TX(X^TA^TX)^{-1})

\tfrac{\partial ((Xa + b)^T C (Xa + b))}{\partial X} = (C+C^T)(Xa+b)a^T

}

\section*{Linear Regression}
\label{sec-2}
\setlength{\parindent}{0pt}
{\scriptsize
Assumes no multicollinearity, homoscedacity, normally distributed data

Risk = R[w] = \Sigma_k l_k(x^k, y^k) = ||Xw^T - y||^2 = wX^TXw^t - 2 w X^Ty + y^Ty

\hat{w} = (X^TX)^{-1}X^T y

Loss = l_k = (x^k w - y^k)^2

Gradient = \nabla_w R = 2(X^TXw^T -X^T y)

f(x) = w \cdot x + b

\noindent\rule{8cm}{0.4pt}

Ridge regression = MAP estimate with a gaussian prior.

Risk = R[w] = \Sigma_k w \phi(x^k) - y^k)^2 + \lambda ||w||_2

w = (X^TX + \lambda I)^{-1}X^T y

Loss = l_k = (x^k w - y^k)^2

Overdetermined = N >> d, Underdetermined = d >> N => invert whatever is smaller b/c pseudo-inverse is equal as \lambda \rightarrow 0

(X^TX + \lambda I)^{-1}X^T y = X^T(XX^T + \lambda I) y

\noindent\rule{8cm}{0.4pt}

Lasso Regression = MAP estimate with a laplace prior.

Risk = R[w] = \Sigma_k w \phi(x_k) - y^k)^2 + \lambda ||w||_1

Loss = l_k = (x^k w - y^k)^2

}

\section*{Logistic Regression}
\label{sec-3}
\setlength{\parindent}{0pt}
{\scriptsize
Good: Soft + flexible + modular decisions, probabilistic output, change decision boundary manually.  We model the decision boundary as f(x). Logistic function S is a mapping from [-\inf, \inf] \rightarrow [0, 1].

In logistic regression, only the marginal examples significantly contribute to determining the position and slope of the sigmoid. LR can be motivated from a generative model with Gaussian or poisson class conditionals.

f(x) = w \cdot x + b

Logistic Function: S(t) = g^{-1}(t) = \tfrac{1}{1+e^{-z}}

S(f(x)) = P_f(Y=1 | X=x)

Logit Link Function: g(p) = log(\tfrac{p}{1-p})

Link Function: S^{-1} = g, f(x) = g(p)

Linear Logistic regression (log odds ratio/logit): log(\tfrac{P_f(Y=1|X=x)}{P_f(Y=-1|X=x)}) = w \cdot x + b

Non-Linear Logistic regression (log odds ratio/logit): log(\tfrac{P_f(Y=1|X=x)}{P_f(Y=-1|X=x)}) = f(x)

Risk(Cross-Entropy) = R(f) = (1/N) \Sigma_k = -log(P_f(Y=y^k | X=x^k))

Logistic Loss == Cross-Entropy Loss

P(Y = 1 | X = x) = \tfrac{1}{1 + e^{-f(x)}}, P(Y = -1 | X = x) = \tfrac{1}{1 + e^{f(x)}}

Functional margin, (y = +/- 1), z = y \cdot f(x)

P(Y=y | X=y) = \tfrac{1}{1 + e^{-z}}

- log(P(Y=y | X= x)) = log( 1 + e^{-z})

-S(-z) = -e^{-z} / (1 + e^{-z})

Kernelized Logistic Regression: \Delta w \approx s(-z_k) y_k \phi(x^k), \Delta \alpha_k ~ S(-z_k)y_k

}

\section*{Hebb's Rule}
\label{sec-4}
\setlength{\parindent}{0pt}
{\scriptsize
Super simple method for classification.

Centroid method = f(x) = (\mu_1 - \mu_0) \cdot x + b = 0

To get descriminant function: f(x) = w\cdot x = 0

w = \sum_k \phi(x)^k y^k

Weights: w_i \leftarrow w_i + y^k x^k_i, \text{Weights w/ decay:} w_i \leftarrow (1-\gamma)row_i + y^k x^k_i, \gamma \in [0, 1]

f(x) = w \cdot x = (np) = \Sigma_k = y^k x^k \cdot x
}

\section*{Perceptron}
\label{sec-5}
\setlength{\parindent}{0pt}
{\scriptsize
Given linearly seperable data, the percepttrion algorithm will take no more than R^2/\gamma^2 \text{updates to converge. Where} R = max_i ||x||_i \text{is the radius of the data and } \gamma = min_i \tfrac{y_i(\alpha \cdot x_i)}{||\alpha||}

Risk = R[w] = \sum_k l(x^k, y_k), \text{ Loss } = max(0, -z), f(x) = sign(\Sigma_k \alpha_i \phi(x_k))

z = y \cdot f(x) = \Sigma_i w_i y \phi_i(x)

Hard Margin: m = 1 / ||w||

Optimum margin: \Delta w_i = \eta y x_i \text{ if incorrect else 0}

Optimum margin: \Delta \alpha^k = \eta y^k x_k \text{ if incorrect else 0}

Geometric(1/||m|| and functional margin link: w \cdot x_1 + b = 1, w \cdot x_2 + b = -1

w (x_1 - x_2) = 2

Dist x_1, x_2 = 2m = ||x_1 - x_2|| = (x_1 - x_2) \cdot w/||w|| = \tfrac{2}{||w||}}

\noindent\rule{8cm}{0.4pt}

Support vectors are the examples closest to the decision boundary. No matter the dimensions, the minimum number of required support vectors is 2. Good fit = allow a few training errors, good robustness = maximize the margin for a classifier. Slackness = every training point misclassified by a soft-margin SVM is support vector. We need only the dot product of x_i, x_j \forall i,j.\text{ and no more info.}

Strong duality holds for HM and SM SVM.

Soft Margin = min \tfrac{1}{|m|} + C * R_{train}, \text{Hard Margin } = min C * R_{train},

Soft Margin SVM = as C \rightarrow 0, \text{ width of margin } \tfrac{2}{||w||} \rightarrow \inf

Large C = focus on a fit to the data, small margin is OK[hard margin]. Small C = focus on large margin, less tendency to overfit.

}

\section*{Gradient Descent}
\label{sec-6}
\setlength{\parindent}{0pt}
{\scriptsize

\begin{tabular}{ l l }
GD: \partial Risk / \partial w & SGD: \partial Loss / \partial w \\
w_{t+1} = w_t - \gamma (1/n) \Sigma_n \nabla_w l(f_w(x^k), y) & w_{t+1} = w_t - \gamma_t \nabla_w l(f_w(x^k), y) \\
\Delta w_{gd} = - \eta \nabla_w R - \gamma w & \Delta w_{sgd} = - \eta \nabla_w L - \gamma w \\
\end{tabular}

\noindent\rule{8cm}{0.4pt}

Regression Example

R[w] = \Lambda (Xw - Y)^2 = (Xw - Y)^T \Lambda (Xw - Y)

= w'X' \Lambda XW - 2y' \Lambda Xw - y' \Lambda y

0 = \dfrac{ \partial w'X' \Lambda Xw}{\partial w} - 2 \dfrac{\partial y' \Lambda Xw}{\partial w} - \dfrac{ \partial y' \Lambda y}{\partial w} = \dfrac{ \partial w'X' \Lambda Xw}{\partial w} - 2 \dfrac{\partial y' \Lambda Xw}{\partial w}

0 = \dfrac{ \partial w' \beta w}{\partial w} - 2 \dfrac{\partial \alpha w}{\partial w}, \beta = X' \Lambda X, \alpha = y' \Lambda X

0 = (\beta + \beta^T)w  - 2 \alpha^T = 2X' \Lambda Xw - 2X' \Lambda y

w = (X' \Lambda X)^{-1} X' \Lambda y

R[w] = \Lambda (Xw - y)^2 + \gamma w'w

0 = 2X' \Lambda Xw - 2X' \Lambda y + \dfrac{\partial \gamma w'w}{\partial w} = 2X' \Lambda Xw - 2X' \Lambda y + 2 \gamma w

w = (X' \Lambda X + \gamma I)^{-1} X' \Lambda y

With Weight decay: \alpha_h^{(t+1)} = \alpha_h^{(t)} - \eta\gamma\alpha_h^{(t)}\quad(\text{For other examples }h\in\{1,2\dots,m\}/i)

\noindent\rule{8cm}{0.4pt}

Loss Functions:

Losses as an expression of a function of the functional margin (z), z = y f(x)
\begin{tabular}{ l l }
  Zero-One  & L(z) = 1(Sign(f(x)) \neq y) = 1(z) \leq 0) \\
  Square  & L(z) = (f(x) - y)^2 = (1 - z)^2, y = +/- 1 \\
  Hinge  & max(0, 1-z) (svc loss) \\
  Logistic  & log(1 + e^{-z}) \\
Perceptron  & max(0, -z) \\
\end{tabular}


}

\section*{Kernels}
\label{sec-7}
\setlength{\parindent}{0pt}
{\scriptsize
Kernel = similarity measure, a dot product in some feature space

What makes a good kernel? Symmetric, Kernel matrix K is invertible (satisfies Mercer's condition), Kernel matrix is PSD if eigenvalues are positive or if it is an outer product. Classifier: f(x) \Sum_k \alpha^k k(x^k, x) + b

Gaussian Kernel is a subset of RBF kernel

Gaussian: k(s, t) = e^{-||s - t||^2 / \sigma^2}, \text{Polynomial:} k(s, t) = (s . t)^q

([s_1, s_2] \cdot [t_1, t_2])^2 = [s_1^2, s_2^2, \sqrt{2}s_1,s_2] \cdot [t_1^2, t_2^2, \sqrt{2}t_1t_2]

K(x,y) = \left(\sum_{i=1}^n x_i y_i + c\right)^2 =

\sum_{i=1}^n \left(x_i^2\right) \left(y_i^2 \right) +
\sum_{i=2}^n \sum_{j=1}^{i-1} \left( \sqrt{2} x_i x_j \right)
                              \left( \sqrt{2} y_i y_j \right)
+ \sum_{i=1}^n \left( \sqrt{2c} x_i \right) \left( \sqrt{2c} y_i \right) + c^2

\noindent\rule{8cm}{0.4pt}

kernel Machines + Dual Representation (parametric, non-parametric)

f(x) = w \phi(x) = \Sigma_k \alpha_k k(x^k, x), w = \Sigma_k \alpha_k \phi(x^k) , k(x^k x) = \phi(x^k)\phi(x)

\noindent\rule{8cm}{0.4pt}

Parzen windows = assign x to the class label of the majority of the examples enclosed in a sphere of radius \sigma

f(x) = \Sigma_k y_k k(x, x_k)

Parzen window for the linear kernel is just Hebb's rule, k(x, x_k) = x . x_k

\noindent\rule{8cm}{0.4pt}




}

\section*{ML \& MAP}
\label{sec-8}
\setlength{\parindent}{0pt}
{\scriptsize
max likelihood, a point estimate, not a dist. Maximize probability given the model,  = \product P(X|\theta) = \Sigma log(P(X|\theta))

if f(x;\theta) = PMF, \text{ then } l(f) < 1

\noindent\rule{8cm}{0.4pt}

Maximum Likelihood for exponential distribution.

P(x_i | \theta) = \theta e^{-\theta x}

lik(\theta) = \prod^n p(x_i |\theta) = \sum_{i=1}^n log(p(x_i |\theta)) = \sum_{i=1}^n log(\theta) - \theta x_i = n log(\theta) - \sum_{i=1}^n \theta x_i

max. lik(\theta) = \partial y / \partial \theta = 0

\tfrac{\partial n log(\theta) - \sum_{i=1}^n \theta x_i}{\partial \theta} = 0

n/\theta - \sum_{i=1}^n x_i = 0, \theta = \tfrac{\sum_n x_i}{n}

\noindent\rule{8cm}{0.4pt}

Maximum a posteriori: Maximize the model given the data. Use bayes rule to invert the ML formula. The MAP estimate allows us to inject into the estimation calculation our prior beliefs regarding the parameters values in \Theta.

= argmax(P(model|data)) = argmax(P(data|model)P(model)) / P(data) (P(data) is constant, so we remove it)

= argmax(P(model|data)) = argmax(P(data|model)P(model)) = argmin -log(p(d|m)) - log(p(m)) = - log likelihood - log prior

\noindent\rule{8cm}{0.4pt}

Prob. of Linear Regression: P(y|x,\sigma^2) \approx N(w^Tx,\sigma^2) \text{ with prior } p(w).

Assume Laplace, show equivalent to minimizing R(w) = \Sigma_n(y_k - w^T x^k)^2 + \lambda ||w||_1

w_j \approx Laplace(0,t), P(w_j) = \tfrac{1}{2t} e ^{-|w_j|/t}, P(w) = \product P(w_j) = (\tfrac{1}{2t})^D \cdot e^{- (\Sigma|w_j|)/t}

P(w|X_i,Y_i) \propto (\prod N(Y_i | w^T X_i, \sigma^2)) \cdot P(w)

l(w) = \Sigma log(N("")) + \Sigma log P(w_j)

= - \Sigma \tfrac{Y_i - w^TX_i)^2}{2\sigma^2} + \tfrac{-\Sigma |w_j|}{t} + nlog \tfrac{1}{\sqrt{2\pi\sigma^2}} + Dlog(1/(2t))

First two terms are our only w terms, last two are constants.

= \Sigma(y_i - w^T x_i)^2 + 2 \sigma^2 / t \Sigma |w_j|

The last term becomes the lambda in front, proving that it's equal to the above Risk function.

}
\section*{Gaussian Classifiers, PCA \& LDA}
\label{sec-9}
\setlength{\parindent}{0pt}
{\scriptsize

Gaussian Classifier = P(X=x | Y=y) \propto e^{||X-\mu^{class,y}||^2 / 2 \sigma^2}

Isotropic Gaussian model: patterns x are generated from a template (class centroid) plus some gaussian noise with 0 mean and same variance. Shrunken centroid method takes all this a bit father by rescaling and selecting the most informative features.

Isotropic Gaussians (Same var, in all directions): f(x) = (\mu^1 - \mu^0) \cdot x + b, b = (\mu^{0,2} - \mu^{1,2}) / 2 + log(N_1/N_0)

Correct if different scales of features(Sphering): f(x) = (\mu^1 / \sigma - \mu^0 / \sigma) \cdot x + b, b = ((\mu/\sigma^2)^{0,2} -
(\mu/\sigma^2)^{1,2}) / 2 + log(N_1/N_0)

Most general case(LDA, this is equivalent to whitening): f(x) = \Sigma^{-1} (\mu^1 - \mu^0) \cdot x + b, b = ((\mu^0 \Sigma^{-1} \mu^0 - \mu^1 \Sigma^{-1} \mu^1) / 2 + log(N_1/N_0)

LDA is a generalization of the Gaussian classifer for cases in which the input variables are not statistically independent, but all classes have the same covariance matrix. Once we rotate the input space into the principal axes of the covariance matrix and rescale by the eigen values, LDA is like the centroid method.

P(X=x|Y=y) \propto exp(-1/2 ( x - \mu^y) \Sigma^-1 (x-\mu^y)^T

When we do not hold the covariance of the classes as constant, we get QDA.

Maximize \tfrac{\mu_1^2 - \mu_0^2}{\sigma^2}, \text{ new } f(x) = \Sigma^{-1}(\mu_1^2 - \mu_0^2) \cdot x + b

Pooled within class Covariance Matrix = \Sigma_{LDA}

After that the process for LDA is to use the centroid method.

When we use shrinkage with LDA and have balanced binary classes of -1 and 1, it can be shown that the decision boundary created is exactly equal to the ridge regression of the data. Except for differing covariance matrices, where ridge regression used whole covariance.

\noindent\rule{8cm}{0.4pt}

Mixture Models: Mixed clusters of data. Some smaller, some larger for a given class.

f(x) = \sum_k \alpha_k k(\mu^k, x;\sigma)

P(X=x|y) = \Sigma_k P(X=x, S=s_k |Y=y) P(Y=y)

= \Sigma_k P(X=x, S=s_k |Y=y) P(S=s_k|Y=y) P(Y=y)

\Sigma_k P(X=x, S=s_k |Y=y) \propto exp(- ||x-\mu_k||^2/2\sigma^2)

P(S=s_k|Y=y) P(Y=y) \propto \alpha_k

\noindent\rule{8cm}{0.4pt}

PCA = eigenvectors of covariance matrix with large eigenvalues.PCA is a method of feature construction. The new features are linear combinations (weighted sums) of the old ones. They are obtained by rotating the input space into the axes of the principal components of XTX: X->XU, where the columns of U are eigenvectors.  This transform has the following properties: (1) the eigen directions corresponding to the largest eigenvalues explain best the variance in the data; (2) If we limit ourselves to the n' eigen directions corresponding to the top eigenvalues and rotate back into the original axes: XU -> XUUT, the reconstructed data XUUT are closest to the original data X in the least square sense. So we cut down on the number of features with as small as possible information loss.

Total Covariance Matrix = \Sigma_{PCA}

}

\section*{Generative and Discriminative}
\label{sec-10}
\setlength{\parindent}{0pt}
{\scriptsize

Generative = model P(Y) P(X|Y) as prior and maximum likelihood/maximum a posteriori. Use those to generate P(Y|X).

Discriminitive = model P(Y|X) directly \rightarrow P(X,Y) = P(Y|X)P(X)

A discriminant function f(x) is a function such that f(x) > 0 for 1 class and f(x) < 0 for the other. f(x) = 0 is the equation of the decision boundary. Given w, f(x) = wx is a linear discriminant function. Corresponding decision boundary w.x=0 is a hyperplane (a subspace of dimension (d-1)). We can transform x into another space to get non-linear decision boundaries.
}

\section*{Risk, Loss \& SRM}
\label{sec-11}
\setlength{\parindent}{0pt}
{\scriptsize
Risk = Sum of the losses, Risk function = convex if Hessian = PSD

Three ways to reduce risk and get an idea about your error term. You can either subset different models or you can use shrinkage to reduce the effective dimensions of one kind of model or you could choose more complex kernel subsets.

Empirical Risk is the average loss over a finite number of given examples, Empirical Risk: r_{train}[f] = (1/N) \Sigma_k L(f(x^k),y)

Expected Risk: The expected value of the loss, i.e. the average over an infinite number of examples.

Expected Risk/Generalization Error: R[f] = \int L(f(x, w), y) dP(x, y)

The problem is, we don't know that P(x, y) we only get test examples to get Empirical Risk: R_{test}[f] = (1/N) \Sigma_k L(f(x^k),y)

Guaranteed Risk: Upper bound on the expected risk, measure of training risk + some pre-determined error bar that's a function of the complexity and number of examples. As training error decreases and model complexity increases, we are reaching the guaranteed risk.

R_{gua}[f] = R_{train}[f] + \ep(\delta, C/N) , R[f] \leq r_{gua}[f]

MAP is a method of SRM where -loglike = emp. risk, regularizer = negative log prior
}

\section*{Scoring and Evaluation}
\label{sec-12}
\setlength{\parindent}{0pt}
{\scriptsize
Error rate = E, Accuracy = A: A = 1 -E

BAC = 1/2 * (\tfrac{tp}{tp+fn}  + \tfrac{tn}{tn+fp})

Balanced Error Rate (BER), Balanced Accuracy (BAC): BAC = 1 - BER

AUC for ROC: y = Positive Class Success Rate, x = false alarms

Mean Squared Error or the R^2 = 1 - RSS/TSS

\noindent\rule{8cm}{0.4pt}

Error Rate is binomial:  f(k;n,p) = \Pr(X = k) = \binom n k  p^k(1-p)^{n-k}

Error Bars, p + (1-p) = 1, binomially distributed. Expected = np, Variance = np(1-p). Expected value of error rate E = x\n = p

Error bar of error rate with n test examples, \sigma = \sqrt{E(1-E)/n}

Use the bootstrap rather than knowing the distribution of our favorite cost function. Resample with replacement.

\noindent\rule{8cm}{0.4pt}

Bonferroni Correction: New necessary p-value = n-trials * p-value

}

\section*{Model Search}
\label{sec-13}
\setlength{\parindent}{0pt}
{\scriptsize
Lots of hyperparameters to tune, preprocessing, model, model hyperparameters, loss function, regularizer, learning rate and more.

Brute force / Grid Search: Simple, global minima, but scales poorly. Can do fancy versions like simulated annealing and random walks but these are fairly intensive/complicated methods. Greedy search chooses a random direction and walks until a minimum is reached then tries another value.

\noindent\rule{8cm}{0.4pt}

Filter Methods: Leverage knowledge in order to simplify the search base.

\noindent\rule{8cm}{0.4pt}

Wrapper Methods: your machine = black box and you only tune hyperparameters. ie CV.

\noindent\rule{8cm}{0.4pt}

Embedded Methods: push hyperparameters down to the model level. Wrappers are very computationally expensive and overfit so we try and reduce them down by pushing to the model level. We need two levels of inference to enjoy finite capacity of the learning problem. Optimizing the kernel parameters leads to infinite VC dimension (can learn perfectly any training set); optimizing the ridge or regularization parameters leads to zero capacity.

}

\section*{General Concepts}
\label{sec-14}
\setlength{\parindent}{0pt}
{\scriptsize

Cross-validation is used to select hyperparameters and prevent overfitting.

Cross-entropy = - log likelihood. A way of comparing distributions. KL Divergence is a way of comparing probability distributions.

Normal equations can be derived from minimizing empirical risk, assuming that P(y|X=x) is normally distributed with mean w^Tx \text{ and variance } \sigma^2, \text{assuming that }y = w^Tx + \epsilon, \text{ where } \epsilon \approx N(0,\sigma^2)

isocontours of Gaussian distribution have axes whose lengths are proportional to the square root of eigenvalues of the covariance matrix.

Learning Rate != hyperparameter.

\noindent\rule{8cm}{0.4pt}

Bayes Rule = P(X,Y) = P(X)P(Y|X) = P(Y)P(X|Y), Posterior is proportional to the prior and the likelihood.

Bayes Optimal Discriminant Classifier/Optimal Decision Boudary for unknown P(X,Y) = f*(x) = P(Y=1|X=x) = P(Y=-1|X=x)

Naive Bayes Classifier: argmax_y \prod P(X=x_i | Y=y_i)P(Y=y)

Bayesian learning: one seeks to estimate for a new example x the probability P(y|x,D) by integrating over all possible choices of f, using P(f|D): P(y|x,D) = \int P(y|x,f) dP(f|D).

\noindent\rule{8cm}{0.4pt}

PDF/CDF Information

Expected Value: Get CDF, multiply by values

= \int \tfrac{2}{\pi(1+x^2)}dx
= \tfrac{2}{\pi}\int \tfrac{2}{1+x^2}dx
= \tfrac{2}{\pi}tan^{-1}(x)

tan^{-1}(0) = 0,
tan^{-1}(1/\sqrt{3}) = \pi/6,
tan^{-1}(1) = \pi/4,
tan^{-1}(\sqrt{3}) = \pi/3

\int_a^b f(x)dx = F(b) - F(a)

For the interval from 0 -> 1/\sqrt{3},
2/\pi * \pi/6 - 0 = 2/6

For the interval from 1/\sqrt{3} -> 1,
2/\pi * \pi/4 - 2/\pi * \pi/6 = 1/6

For the interval from 1 -> \sqrt{3},
2/\pi * \pi/3 - 2/\pi * \pi/4 = 1/6


Now that we have those values we can go about calculating the expectation which is simply the probabilities of each of those happening (that we calculated above) multiplied by the point values in order to get the expected value.

4/6 + 3/6 + 2/6 = \tfrac{13}{6}

\noindent\rule{8cm}{0.4pt}

VC Dimension: A measure of the capacity (complexity, flexibility) of a model. C = number of training examples that can be seperated, regardless of label assignment. For linear, C = d

\noindent\rule{8cm}{0.4pt}

Leave One Out Cross validation: By repeating this we can get a bound on our error rate. Virtual leave one out is a derivation of a learning method to do this at train time so that we get this error bound for free. Involved in sensitivity analysis, how sensitive is our model?


\noindent\rule{8cm}{0.4pt}

Fit vs Robustness

Best fit = no training error, Best robustness = based on typical examples

\noindent\rule{8cm}{0.4pt}

PSD = x^TAx \geq 0, \text{Symmetric PSD matrix != all non-negative elements}

PSD diagonals = non-negative ? CLEAN ME

\noindent\rule{8cm}{0.4pt}

Eigenvectors/Values = Av = \lambda v, (A -\lambda I)v = 0

\noindent\rule{8cm}{0.4pt}

X is Centered, COV(X) = X^TX = USSU^T

X is not Centered, COV(X) = \Sigma=\mathrm{E}
\left[
 \left(
 \mathbf{X} - \mathrm{E}[\mathbf{X}]
 \right)
 \left(
 \mathbf{X} - \mathrm{E}[\mathbf{X}]
 \right)^{\rm T}
\right]

X^TX = US^2U^T, dim(u) = (d, r_rank), dim(s) = (r_rank, r_rank)

XX^T = VS^2V^T, dim(v) = (N, r_rank), dim(S) = (r_rank, r_rank)

U = eigenvectors, s = singular diagonalized singular values, s^2 = eigenvalues

*Application to Ridge Regression* - if we want to vary lambda a lot or try a lot of different lambdas, it's worth it to perform this rotation because it will make it much easier to both invert and add to the diagonals because we've got this diagonalized matrix. Makes it a bit easier to manipulate.

\noindent\rule{8cm}{0.4pt}

Independence = P(X,Y) = P(X)P(Y), C(x,y) = ( x-\mu_x ).( y-\mu_y )/ (sx sy)

\noindent\rule{8cm}{0.4pt}

U and V are unitary rotational matrices, Scaling Matrix = \Sigma, SVD(X) = U \Sigma V^T

\noindent\rule{8cm}{0.4pt}

if Centered Data, \Sigma = X^TX, \text{ Centering: } x^k - \mu

To rotate: XU, U from \Sigma^1=UDU^T

Standardizing/ Sphering: (x^k - \mu) / \sigma, \text{ Whitening: }\sqrt{COV(X)^{-1}}  = \Sigma ^{-1/2}

Whitened Space(and how to send data to it): \phi = X \Sigma ^{-1/2}

\noindent\rule{8cm}{0.4pt}

Log Rules: log(x^y) = y \cdot log(x)

If X1 and X2 are normal + ind. (X1,X2) must have mv normal dist. => true. If (X1,X2) = mv normal dist, not independent.

guassian: P(x_i|\mu_i,\sigma_i) = \tfrac{1}{(\sigma_i \sqrt{2\pi}\sigma)} e^{- \tfrac{(x-\mu)^2}{2\sigma^2}

Log likelihood of guassian: l(X_1,...X_N; \mu, \sigma) = N ln(\tfrac{1}{\sqrt{2\pi}\sigma}) - \tfrac{1}{2\sigma^2}\sum (X_i - \mu)^2

\mu = 1/N\sum X_i, \sigma = \sqrt{(1/N) \sum(X_i - \mu)^2}
}



























\setlength{\parindent}{0pt}
{\scriptsize
}
% Emacs 24.4.1 (Org mode 8.2.10)
\end{document}
